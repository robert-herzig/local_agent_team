# AI Search Chat Web Interface

A sophisticated web-based chat interface for your AI system with search enhancement, conversation memory, image generation, and **fully operational RAG (Retrieval-Augmented Generation)** capabilities.

## üöÄ Quick Start with Lightweight Node.js (Recommended)

**Memory-efficient deployment using Node.js!** Only ~250MB RAM vs Docker's 2-4GB:

### Prerequisites
- Node.js LTS installed (download from: https://nodejs.org/)
- Python with conda environment (for backend)
- Ollama running with your preferred models

### Easy Setup (Windows)
1. **Double-click `start-lightweight.bat`** - This will automatically:
   - Check Node.js installation
   - Install frontend dependencies
   - Start both backend and frontend
   - Open the application in your browser

2. **Access the application:**
   - Frontend: http://localhost:3000 ‚úÖ
   - Backend API: http://localhost:8000 ‚úÖ  
   - API Docs: http://localhost:8000/docs ‚úÖ
   - RAG Documents: http://localhost:8000/rag/documents ‚úÖ

3. **Memory usage comparison:**
   - Node.js Setup: ~250MB RAM üéØ
   - Docker Setup: 2-4GB RAM üíæ

**‚ö†Ô∏è Important:** If you encounter "RAG functionality not available" error:
1. Install missing dependencies: `conda activate ai-chat && pip install tiktoken openai`
2. Restart the backend server
3. The RAG system will then initialize automatically

### Alternative: Docker Setup
For users who prefer containerized deployment:
```bash
# Development mode (with hot reloading) 
docker-compose -f docker-compose.dev.yml up --build

# Production mode
docker-compose up --build

# Stop services
docker-compose -f docker-compose.dev.yml down
```

## Architecture Overview

### Backend (FastAPI)
- **Real-time Communication**: WebSocket support for live chat updates
- **REST API Fallback**: HTTP endpoints for reliable communication  
- **Async Processing**: Non-blocking search and AI operations
- **Session Management**: Persistent conversation history
- **Image Generation**: Stable Diffusion integration
- **RAG System**: ‚úÖ **FULLY OPERATIONAL** - PDF processing, vector database, and hybrid search

### Frontend (React)
- **Modern UI**: Clean, responsive chat interface
- **Real-time Updates**: WebSocket integration with fallback
- **Markdown Support**: Rich text rendering with syntax highlighting
- **Image Display**: Generated image preview and gallery
- **Document Upload**: ‚úÖ **WORKING** - Drag-and-drop PDF upload
- **Mobile-Friendly**: Responsive design for all devices

## Setup Instructions

### 1. Lightweight Node.js Setup (Recommended - ~250MB RAM)

#### Quick Start:
```bash
# Run the automated setup script
start-lightweight.bat
```

#### Manual Setup:
```bash
# 1. Backend Setup (Python with conda)
cd backend
conda create -n ai-chat python=3.10
conda activate ai-chat
pip install -r requirements.txt

# Start backend
python start_server.py
```

```bash
# 2. Frontend Setup (Node.js)
cd frontend
npm install

# Start frontend  
npm start
```

### 2. Alternative Docker Setup (Higher Memory Usage)

```bash
cd backend
pip install -r requirements.txt
```

### 3. Running the Application

#### Lightweight Mode (Node.js):
- **Backend**: Automatically started with `start_server.py` on port 8000
- **Frontend**: React development server on port 3000
- **Memory**: ~250MB total RAM usage

#### Docker Mode:
- Use `docker-compose -f docker-compose.dev.yml up --build`
- **Memory**: 2-4GB RAM usage

## Features

### ‚úÖ Currently Available & Fully Operational
- **Multi-Model Architecture**: Different models for different tasks
- **Intelligent Search Routing**: Automatic decision on when to search
- **Web Search Integration**: DuckDuckGo and Google search with content extraction
- **Conversation Memory**: Persistent chat history with context awareness
- **Image Generation**: Stable Diffusion WebUI integration
- **Real-time Interface**: WebSocket communication with status updates
- **Source Attribution**: Display of search sources and queries used
- **RAG System**: ‚úÖ **FULLY WORKING** - Complete document processing pipeline
- **PDF Upload**: ‚úÖ **Drag-and-drop document upload working**
- **Document Processing**: ‚úÖ **Text extraction and chunking operational**
- **Vector Database**: ‚úÖ **ChromaDB with semantic search active**
- **Hybrid Search**: ‚úÖ **Combines web search with document knowledge**
- **Document Management**: ‚úÖ **List, organize, and manage uploaded files**

### üîß Technical Implementation Complete
- **ChromaDB Integration**: Vector database for document embeddings
- **Sentence Transformers**: all-MiniLM-L6-v2 model for embeddings
- **Async Document Processing**: Non-blocking PDF/DOCX/TXT processing
- **Memory Optimization**: ~250MB RAM usage vs Docker's 2-4GB

## Technical Details

### WebSocket Communication
The interface uses WebSocket for real-time communication with fallback to REST API:
- Real-time status updates during processing
- Streaming responses for better user experience
- Automatic reconnection handling

### Search Enhancement Pipeline
1. **Knowledge Assessment**: Check if AI can answer without search
2. **Query Generation**: Create optimized search queries
3. **Web Search**: Multi-source search with anti-bot protection
4. **Source Selection**: AI-powered selection of best sources
5. **Content Extraction**: Clean text extraction from web pages
6. **Answer Synthesis**: Combine sources with conversation context

### Image Generation Pipeline
1. **Request Detection**: Identify image generation requests
2. **Prompt Extraction**: Extract and enhance image descriptions
3. **API Integration**: Connect to Stable Diffusion WebUI
4. **Image Storage**: Save and serve generated images
5. **Fallback Handling**: Mock responses when API unavailable

## Configuration

### Environment Variables
Create a `.env` file in the backend directory:
```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Stable Diffusion WebUI
SD_WEBUI_URL=http://127.0.0.1:7860

# RAG Configuration (Active)
VECTOR_DB_PATH=./vector_db
UPLOAD_PATH=./uploads
MAX_FILE_SIZE=50MB
CHROMADB_PERSIST_DIR=./vector_db
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

### Model Configuration
Edit `prompts.json` to customize AI behavior:
- Search decision prompts
- Query generation instructions
- Source selection criteria
- Response generation style

## RAG System Implementation (‚úÖ COMPLETE)

The RAG system is now fully operational and integrated:

### 1. Document Processing Pipeline
```python
# Implemented and working
async def process_pdf(file: UploadFile):
    # ‚úÖ Extract text from PDF using PyPDF2/pdfplumber
    # ‚úÖ Split into chunks with overlap
    # ‚úÖ Generate embeddings with sentence-transformers
    # ‚úÖ Store in ChromaDB vector database
```

### 2. Hybrid Search (‚úÖ Active)
```python
# Live implementation
async def hybrid_search(query: str):
    web_results = await web_search(query)
    doc_results = await vector_search(query)
    return combine_results(web_results, doc_results)
```

### 3. Vector Database Integration (‚úÖ Operational)
- **ChromaDB**: Document embeddings and similarity search
- **Sentence Transformers**: all-MiniLM-L6-v2 for embedding generation
- **Async Processing**: Non-blocking document operations
- **Persistent Storage**: SQLite metadata + ChromaDB vectors

## Development Notes

### Adding New Features
1. **Backend**: Add endpoints in `backend/main.py`
2. **Frontend**: Create components in `frontend/src/components/`
3. **Models**: Update `prompts.json` for new AI behaviors

### Testing
- Backend: `pytest` (add tests in `backend/tests/`)
- Frontend: `npm test`

### Deployment
- Backend: Can be deployed with Docker or directly with uvicorn
- Frontend: Build with `npm run build` and serve statically
- Database: Add PostgreSQL or MongoDB for production

## API Endpoints

### WebSocket
- `ws://localhost:8000/ws/{session_id}` - Real-time chat

### REST API
- `POST /chat` - Send message (web search + RAG)
- `POST /chat-rag` - RAG-enhanced chat responses
- `GET /sessions/{session_id}/history` - Get chat history
- `DELETE /sessions/{session_id}` - Clear session
- `GET /images/{filename}` - Serve generated images
- `POST /upload-pdf` - ‚úÖ Upload PDF documents
- `GET /documents` - ‚úÖ List uploaded documents
- `GET /rag/documents` - ‚úÖ RAG document management
- `GET /rag/stats` - ‚úÖ RAG system statistics
- `POST /search` - ‚úÖ Hybrid search endpoint
- `DELETE /documents/{document_id}` - ‚úÖ Delete documents

## Troubleshooting

### Backend Issues
- Ensure Ollama is running and models are available
- Check if Stable Diffusion WebUI is running with `--api` flag
- Verify all Python dependencies are installed
- For RAG issues: Check ChromaDB initialization in terminal output
- Conda environment: Run `conda activate ai-chat` before starting backend

### RAG System Issues ‚úÖ RESOLVED
**If you see "RAG functionality not available" error:**
1. **Install missing dependencies:**
   ```bash
   conda activate ai-chat
   pip install tiktoken openai
   ```
2. **Restart the backend server**
3. **Verify RAG initialization:** Look for "‚úì RAG components imported successfully" in terminal output
4. **Test RAG endpoints:** Visit http://localhost:8000/rag/documents to verify functionality

### Web Search Content Extraction Issues ‚úÖ RESOLVED
**If web search finds sources but doesn't extract content:**
1. **Check for prompt parameter mismatches** - Fixed: `search_results` vs `sources` parameter
2. **Verify source selection is working** - Check terminal for "Error selecting sources" messages
3. **Test with current events questions** - Should now extract content from Forbes, Reuters, etc.

### Hybrid Search Relevance Issues ‚úÖ RESOLVED
**If document results are included for irrelevant queries:**
1. **Improved relevance scoring** - Document similarity threshold increased to 40% minimum
2. **Stricter confidence calculation** - Multi-factor relevance assessment implemented
3. **Better filtering** - Irrelevant documents automatically excluded from results
4. **Balanced weighting** - Web sources now prioritized (60%) over documents (40%) for current events

**Test Results After Improvements:**
- ‚úÖ Weather questions: Only web sources, no irrelevant documents
- ‚úÖ Current events: Prioritizes web sources from Forbes, Reuters, etc.
- ‚úÖ AI technology questions: Uses relevant web sources, filters out unrelated docs
- ‚úÖ Document-specific queries: Includes documents only when truly relevant (>40% similarity)

### Frontend Issues
- Clear browser cache and restart development server
- Check console for WebSocket connection errors
- Ensure backend is running on port 8000
- Verify Node.js is installed: `node --version`

### Image Generation Issues
- Start Stable Diffusion WebUI with: `./webui.bat --api`
- Check if CUDA/GPU acceleration is working
- Verify PyTorch compatibility with your GPU

## Performance Optimization

### Backend
- ‚úÖ Conda environment for isolated Python dependencies
- ‚úÖ Async document processing with ChromaDB
- ‚úÖ Memory-efficient vector embeddings
- ‚úÖ Lightweight startup script avoiding module conflicts
- Consider Redis for session storage in production
- Add caching for search results

### Frontend
- ‚úÖ Node.js development server (~50MB RAM)
- ‚úÖ Efficient React component structure  
- ‚úÖ WebSocket with HTTP fallback
- Implement virtual scrolling for long conversations
- Add image lazy loading
- Optimize bundle size with code splitting

### Memory Usage Comparison
- **Node.js Setup**: ~250MB total RAM ‚úÖ
- **Docker Setup**: 2-4GB RAM
- **RAG Processing**: ChromaDB optimized for memory efficiency

## Security Considerations

### Production Deployment
- Add authentication and authorization
- Implement rate limiting
- Sanitize user inputs
- Use HTTPS for all communications
- Add CORS configuration for production domains
